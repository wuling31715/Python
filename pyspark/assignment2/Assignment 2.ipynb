{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Version: 3.6\n",
    "* conda create --name py3.6 python=3.6\n",
    "* source activate py3.6\n",
    "* python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Java Version:  8\n",
    "* brew cask install adoptopenjdk/openjdk/adoptopenjdk8\n",
    "* open ~/.bash_profile\n",
    "* export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)\n",
    "* java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 01: Import Spark Session and initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Weather Forecast\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 02: Load the dataset and print the schema and total number of entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('weatherAUS.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary='count', Date='142193', Location='142193', MinTemp='142193', MaxTemp='142193', Rainfall='142193', Evaporation='142193', Sunshine='142193', WindGustDir='142193', WindGustSpeed='142193', WindDir9am='142193', WindDir3pm='142193', WindSpeed9am='142193', WindSpeed3pm='142193', Humidity9am='142193', Humidity3pm='142193', Pressure9am='142193', Pressure3pm='142193', Cloud9am='142193', Cloud3pm='142193', Temp9am='142193', Temp3pm='142193', RainToday='142193', RainTomorrow='142193'),\n",
       " Row(summary='mean', Date=None, Location=None, MinTemp='12.186399728729098', MaxTemp='23.226784191272444', Rainfall='2.3499740743111954', Evaporation='5.469824216349123', Sunshine='7.624853113193571', WindGustDir=None, WindGustSpeed='39.98429165757619', WindDir9am=None, WindDir3pm=None, WindSpeed9am='14.001988000994', WindSpeed3pm='18.63757586179718', Humidity9am='68.8438103105705', Humidity3pm='51.482606091656265', Pressure9am='1017.6537584159781', Pressure3pm='1015.258203537907', Cloud9am='4.437189391885787', Cloud3pm='4.503166899728551', Temp9am='16.98750858170133', Temp3pm='21.68723497314744', RainToday=None, RainTomorrow=None),\n",
       " Row(summary='stddev', Date=None, Location=None, MinTemp='6.403282674671314', MaxTemp='7.117618141018136', Rainfall='8.465172917616425', Evaporation='4.188536508895149', Sunshine='3.7815249942144615', WindGustDir=None, WindGustSpeed='13.588800765487752', WindDir9am=None, WindDir3pm=None, WindSpeed9am='8.893337098234603', WindSpeed3pm='8.803345036235537', Humidity9am='19.051292535336177', Humidity3pm='20.79777184369888', Pressure9am='7.105475711520505', Pressure3pm='7.0366767834928545', Cloud9am='2.8870155257335974', Cloud3pm='2.720632530403652', Temp9am='6.492838325478915', Temp3pm='6.9375938685337335', RainToday=None, RainTomorrow=None),\n",
       " Row(summary='min', Date='2007-11-01', Location='Adelaide', MinTemp='-0.1', MaxTemp='-0.1', Rainfall='0', Evaporation='0', Sunshine='0', WindGustDir='E', WindGustSpeed='100', WindDir9am='E', WindDir3pm='E', WindSpeed9am='0', WindSpeed3pm='0', Humidity9am='0', Humidity3pm='0', Pressure9am='1.00E+03', Pressure3pm='1.00E+03', Cloud9am='0', Cloud3pm='0', Temp9am='-0.1', Temp3pm='-0.1', RainToday='NA', RainTomorrow='No'),\n",
       " Row(summary='max', Date='2017-06-25', Location='Woomera', MinTemp='NA', MaxTemp='NA', Rainfall='NA', Evaporation='NA', Sunshine='NA', WindGustDir='WSW', WindGustSpeed='NA', WindDir9am='WSW', WindDir3pm='WSW', WindSpeed9am='NA', WindSpeed3pm='NA', Humidity9am='NA', Humidity3pm='NA', Pressure9am='NA', Pressure3pm='NA', Cloud9am='NA', Cloud3pm='NA', Temp9am='NA', Temp3pm='NA', RainToday='Yes', RainTomorrow='Yes')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('weatherAUS.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 03: Delete columns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('Date', 'Location', 'Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm')\n",
    "df.show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 04: Print the number of missing data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|    637|    322|    1406|       9330|         9270|     10013|      3778|        1348|        2630|       1774|       3610|      14014|      13981|     1406|           0|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select(*(sum(col(c).isin('NA').cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 05: Fill the missing data with average value and maximum occurrence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+----------------+------------------+------------------+-----------------+\n",
      "|      avg(MinTemp)|      avg(MaxTemp)|     avg(Rainfall)|avg(WindGustSpeed)|avg(WindSpeed9am)|avg(WindSpeed3pm)|avg(Humidity9am)|  avg(Humidity3pm)|  avg(Pressure9am)| avg(Pressure3pm)|\n",
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+----------------+------------------+------------------+-----------------+\n",
      "|12.186399728729098|23.226784191272444|2.3499740743111954| 39.98429165757619|  14.001988000994|18.63757586179718|68.8438103105705|51.482606091656265|1017.6537584159781|1015.258203537907|\n",
      "+------------------+------------------+------------------+------------------+-----------------+-----------------+----------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg('MinTemp'),\n",
    "          avg('MaxTemp'),\n",
    "          avg('Rainfall'),\n",
    "          avg('WindGustSpeed'),\n",
    "          avg('WindSpeed9am'),\n",
    "          avg('WindSpeed3pm'),\n",
    "          avg('Humidity9am'),\n",
    "          avg('Humidity3pm'),\n",
    "          avg('Pressure9am'),\n",
    "          avg('Pressure3pm')).show()\n",
    "\n",
    "avg_list = df.select(avg('MinTemp'), \n",
    "                     avg('MaxTemp'), \n",
    "                     avg('Rainfall'), \n",
    "                     avg('WindGustSpeed'), \n",
    "                     avg('WindSpeed9am'), \n",
    "                     avg('WindSpeed3pm'), \n",
    "                     avg('Humidity9am'), \n",
    "                     avg('Humidity3pm'), \n",
    "                     avg('Pressure9am'), \n",
    "                     avg('Pressure3pm')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+---------------+--------------+-----------------+\n",
      "|max(WindGustDir)|max(WindDir9am)|max(WindDir3pm)|max(RainToday)|max(RainTomorrow)|\n",
      "+----------------+---------------+---------------+--------------+-----------------+\n",
      "|             WSW|            WSW|            WSW|           Yes|              Yes|\n",
      "+----------------+---------------+---------------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "df.select(max('WindGustDir'),\n",
    "          max('WindDir9am'),\n",
    "          max('WindDir3pm'),\n",
    "          max('RainToday'),\n",
    "          max('RainTomorrow'),\n",
    "         ).show()\n",
    "\n",
    "max_list = df.select(max('WindGustDir'),\n",
    "                     max('WindDir9am'),\n",
    "                     max('WindDir3pm'),\n",
    "                     max('RainToday'),\n",
    "                     max('RainTomorrow'),\n",
    "                    ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-----------+-------------+----------+----------+---------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|          Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|   WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+------------------+-----------+-------------+----------+----------+---------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|   13.4|   22.9|               0.6|          W|           44|         W|       WNW|             20|          24|         71|         22|     1007.7|     1007.1|       No|          No|\n",
      "|    7.4|   25.1|                 0|        WNW|           44|       NNW|       WSW|              4|          22|         44|         25|     1010.6|     1007.8|       No|          No|\n",
      "|   12.9|   25.7|                 0|        WSW|           46|         W|       WSW|             19|          26|         38|         30|     1007.6|     1008.7|       No|          No|\n",
      "|    9.2|     28|                 0|         NE|           24|        SE|         E|             11|           9|         45|         16|     1017.6|     1012.8|       No|          No|\n",
      "|   17.5|   32.3|                 1|          W|           41|       ENE|        NW|              7|          20|         82|         33|     1010.8|       1006|       No|          No|\n",
      "|   14.6|   29.7|               0.2|        WNW|           56|         W|         W|             19|          24|         55|         23|     1009.2|     1005.4|       No|          No|\n",
      "|   14.3|     25|                 0|          W|           50|        SW|         W|             20|          24|         49|         19|     1009.6|     1008.2|       No|          No|\n",
      "|    7.7|   26.7|                 0|          W|           35|       SSE|         W|              6|          17|         48|         19|     1013.4|     1010.1|       No|          No|\n",
      "|    9.7|   31.9|                 0|        NNW|           80|        SE|        NW|              7|          28|         42|          9|     1008.9|     1003.6|       No|         Yes|\n",
      "|   13.1|   30.1|               1.4|          W|           28|         S|       SSE|             15|          11|         58|         27|       1007|     1005.7|      Yes|          No|\n",
      "|   13.4|   30.4|                 0|          N|           30|       SSE|       ESE|             17|           6|         48|         22|     1011.8|     1008.7|       No|         Yes|\n",
      "|   15.9|   21.7|               2.2|        NNE|           31|        NE|       ENE|             15|          13|         89|         91|     1010.5|     1004.2|      Yes|         Yes|\n",
      "|   15.9|   18.6|              15.6|          W|           61|       NNW|       NNW|             28|          28|         76|         93|      994.3|        993|      Yes|         Yes|\n",
      "|   12.6|     21|               3.6|         SW|           44|         W|       SSW|             24|          20|         65|         43|     1001.2|     1001.8|      Yes|          No|\n",
      "|    9.8|   27.7|2.3499740743111954|        WNW|           50|       WSW|       WNW|14.001988000994|          22|         50|         28|     1013.4|     1010.3|      Yes|          No|\n",
      "|   14.1|   20.9|                 0|        ENE|           22|       SSW|         E|             11|           9|         69|         82|     1012.2|     1010.4|       No|         Yes|\n",
      "|   13.5|   22.9|              16.8|          W|           63|         N|       WNW|              6|          20|         80|         65|     1005.8|     1002.2|      Yes|         Yes|\n",
      "|   11.2|   22.5|              10.6|        SSE|           43|       WSW|        SW|             24|          17|         47|         32|     1009.4|     1009.7|      Yes|          No|\n",
      "|    9.8|   25.6|                 0|        SSE|           26|        SE|       NNW|             17|           6|         45|         26|     1019.2|     1017.1|       No|          No|\n",
      "|   11.5|   29.3|                 0|          S|           24|        SE|        SE|              9|           9|         56|         28|     1019.3|     1014.8|       No|          No|\n",
      "+-------+-------+------------------+-----------+-------------+----------+----------+---------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df\\\n",
    ".withColumn('MinTemp', when(df.MinTemp=='NA', avg_list[0][0]).otherwise(df.MinTemp))\\\n",
    ".withColumn('MaxTemp', when(df.MaxTemp=='NA', avg_list[0][1]).otherwise(df.MaxTemp))\\\n",
    ".withColumn('Rainfall', when(df.Rainfall=='NA', avg_list[0][2]).otherwise(df.Rainfall))\\\n",
    ".withColumn('WindGustSpeed', when(df.WindGustSpeed=='NA', avg_list[0][3]).otherwise(df.WindGustSpeed))\\\n",
    ".withColumn('WindSpeed9am', when(df.WindSpeed9am=='NA', avg_list[0][4]).otherwise(df.WindSpeed9am))\\\n",
    ".withColumn('WindSpeed3pm', when(df.WindSpeed3pm=='NA', avg_list[0][5]).otherwise(df.WindSpeed3pm))\\\n",
    ".withColumn('Humidity9am', when(df.Humidity9am=='NA', avg_list[0][6]).otherwise(df.Humidity9am))\\\n",
    ".withColumn('Humidity3pm', when(df.Humidity3pm=='NA', avg_list[0][7]).otherwise(df.Humidity3pm))\\\n",
    ".withColumn('Pressure9am', when(df.Pressure9am=='NA', avg_list[0][8]).otherwise(df.Pressure9am))\\\n",
    ".withColumn('Pressure3pm', when(df.Pressure3pm=='NA', avg_list[0][9]).otherwise(df.Pressure3pm))\\\n",
    ".withColumn('WindGustDir', when(df.WindGustDir=='NA', max_list[0][0]).otherwise(df.WindGustDir))\\\n",
    ".withColumn('WindDir9am', when(df.WindDir9am=='NA', max_list[0][1]).otherwise(df.WindDir9am))\\\n",
    ".withColumn('WindDir3pm', when(df.WindDir3pm=='NA', max_list[0][2]).otherwise(df.WindDir3pm))\\\n",
    ".withColumn('RainToday', when(df.RainToday=='NA', max_list[0][3]).otherwise(df.RainToday))\\\n",
    ".withColumn('RainTomorrow', when(df.RainTomorrow=='NA', max_list[0][4]).otherwise(df.RainTomorrow))\\\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 06: Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n"
     ]
    }
   ],
   "source": [
    "column_list = ['MinTemp', \n",
    "               'MaxTemp', \n",
    "               'Rainfall', \n",
    "               'WindGustSpeed', \n",
    "               'WindSpeed9am', \n",
    "               'WindSpeed3pm', \n",
    "               'Humidity9am', \n",
    "               'Humidity3pm', \n",
    "               'Pressure9am', \n",
    "               'Pressure3pm',\n",
    "               'WindGustDir',\n",
    "               'WindDir9am',\n",
    "               'WindDir3pm',\n",
    "               'RainToday',               \n",
    "               'RainTomorrow'\n",
    "              ]\n",
    "\n",
    "for column in column_list:\n",
    "    print(df.schema[column].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df = df\\\n",
    ".withColumn('MinTemp', df['MinTemp'].cast(DoubleType()))\\\n",
    ".withColumn('MaxTemp', df['MaxTemp'].cast(DoubleType()))\\\n",
    ".withColumn('Rainfall', df['Rainfall'].cast(DoubleType()))\\\n",
    ".withColumn('WindGustSpeed', df['WindGustSpeed'].cast(DoubleType()))\\\n",
    ".withColumn('WindSpeed9am', df['WindSpeed9am'].cast(DoubleType()))\\\n",
    ".withColumn('WindSpeed3pm', df['WindSpeed3pm'].cast(DoubleType()))\\\n",
    ".withColumn('Humidity9am', df['Humidity9am'].cast(DoubleType()))\\\n",
    ".withColumn('Humidity3pm', df['Humidity3pm'].cast(DoubleType()))\\\n",
    ".withColumn('Pressure9am', df['Pressure9am'].cast(DoubleType()))\\\n",
    ".withColumn('Pressure3pm', df['Pressure3pm'].cast(DoubleType()))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-----------+-------------+----------+----------+---------------+------------+-----------+-----------+-----------+-----------+---------+------------+----------------+---------------+---------------+--------------+-----------------+\n",
      "|MinTemp|MaxTemp|          Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|   WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|WindGustDirIndex|WindDir9amIndex|WindDir3pmIndex|RainTodayIndex|RainTomorrowIndex|\n",
      "+-------+-------+------------------+-----------+-------------+----------+----------+---------------+------------+-----------+-----------+-----------+-----------+---------+------------+----------------+---------------+---------------+--------------+-----------------+\n",
      "|   13.4|   22.9|               0.6|        1.0|         44.0|       7.0|       7.0|           20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|      0.0|         0.0|             1.0|            7.0|            7.0|           0.0|              0.0|\n",
      "|    7.4|   25.1|               0.0|        9.0|         44.0|      10.0|       0.0|            4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|      0.0|         0.0|             9.0|           10.0|            0.0|           0.0|              0.0|\n",
      "|   12.9|   25.7|               0.0|        0.0|         46.0|       7.0|       0.0|           19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|      0.0|         0.0|             0.0|            7.0|            0.0|           0.0|              0.0|\n",
      "|    9.2|   28.0|               0.0|       13.0|         24.0|       2.0|      10.0|           11.0|         9.0|       45.0|       16.0|     1017.6|     1012.8|      0.0|         0.0|            13.0|            2.0|           10.0|           0.0|              0.0|\n",
      "|   17.5|   32.3|               1.0|        1.0|         41.0|      11.0|       8.0|            7.0|        20.0|       82.0|       33.0|     1010.8|     1006.0|      0.0|         0.0|             1.0|           11.0|            8.0|           0.0|              0.0|\n",
      "|   14.6|   29.7|               0.2|        9.0|         56.0|       7.0|       2.0|           19.0|        24.0|       55.0|       23.0|     1009.2|     1005.4|      0.0|         0.0|             9.0|            7.0|            2.0|           0.0|              0.0|\n",
      "|   14.3|   25.0|               0.0|        1.0|         50.0|       8.0|       2.0|           20.0|        24.0|       49.0|       19.0|     1009.6|     1008.2|      0.0|         0.0|             1.0|            8.0|            2.0|           0.0|              0.0|\n",
      "|    7.7|   26.7|               0.0|        1.0|         35.0|       4.0|       2.0|            6.0|        17.0|       48.0|       19.0|     1013.4|     1010.1|      0.0|         0.0|             1.0|            4.0|            2.0|           0.0|              0.0|\n",
      "|    9.7|   31.9|               0.0|       14.0|         80.0|       2.0|       8.0|            7.0|        28.0|       42.0|        9.0|     1008.9|     1003.6|      0.0|         1.0|            14.0|            2.0|            8.0|           0.0|              1.0|\n",
      "|   13.1|   30.1|               1.4|        1.0|         28.0|       6.0|       5.0|           15.0|        11.0|       58.0|       27.0|     1007.0|     1005.7|      1.0|         0.0|             1.0|            6.0|            5.0|           1.0|              0.0|\n",
      "|   13.4|   30.4|               0.0|        4.0|         30.0|       4.0|       9.0|           17.0|         6.0|       48.0|       22.0|     1011.8|     1008.7|      0.0|         1.0|             4.0|            4.0|            9.0|           0.0|              1.0|\n",
      "|   15.9|   21.7|               2.2|       15.0|         31.0|      13.0|      14.0|           15.0|        13.0|       89.0|       91.0|     1010.5|     1004.2|      1.0|         1.0|            15.0|           13.0|           14.0|           1.0|              1.0|\n",
      "|   15.9|   18.6|              15.6|        1.0|         61.0|      10.0|      13.0|           28.0|        28.0|       76.0|       93.0|      994.3|      993.0|      1.0|         1.0|             1.0|           10.0|           13.0|           1.0|              1.0|\n",
      "|   12.6|   21.0|               3.6|        7.0|         44.0|       7.0|      12.0|           24.0|        20.0|       65.0|       43.0|     1001.2|     1001.8|      1.0|         0.0|             7.0|            7.0|           12.0|           1.0|              0.0|\n",
      "|    9.8|   27.7|2.3499740743111954|        9.0|         50.0|       0.0|       7.0|14.001988000994|        22.0|       50.0|       28.0|     1013.4|     1010.3|      1.0|         0.0|             9.0|            0.0|            7.0|           1.0|              0.0|\n",
      "|   14.1|   20.9|               0.0|       11.0|         22.0|      14.0|      10.0|           11.0|         9.0|       69.0|       82.0|     1012.2|     1010.4|      0.0|         1.0|            11.0|           14.0|           10.0|           0.0|              1.0|\n",
      "|   13.5|   22.9|              16.8|        1.0|         63.0|       1.0|       7.0|            6.0|        20.0|       80.0|       65.0|     1005.8|     1002.2|      1.0|         1.0|             1.0|            1.0|            7.0|           1.0|              1.0|\n",
      "|   11.2|   22.5|              10.6|        5.0|         43.0|       0.0|       4.0|           24.0|        17.0|       47.0|       32.0|     1009.4|     1009.7|      1.0|         0.0|             5.0|            0.0|            4.0|           1.0|              0.0|\n",
      "|    9.8|   25.6|               0.0|        5.0|         26.0|       2.0|      13.0|           17.0|         6.0|       45.0|       26.0|     1019.2|     1017.1|      0.0|         0.0|             5.0|            2.0|           13.0|           0.0|              0.0|\n",
      "|   11.5|   29.3|               0.0|        6.0|         24.0|       2.0|       1.0|            9.0|         9.0|       56.0|       28.0|     1019.3|     1014.8|      0.0|         0.0|             6.0|            2.0|            1.0|           0.0|              0.0|\n",
      "+-------+-------+------------------+-----------+-------------+----------+----------+---------------+------------+-----------+-----------+-----------+-----------+---------+------------+----------------+---------------+---------------+--------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = StringIndexer(inputCol=\"WindGustDir\", outputCol=\"WindGustDirIndex\").fit(df).transform(df)\n",
    "df = StringIndexer(inputCol=\"WindDir9am\", outputCol=\"WindDir9amIndex\").fit(df).transform(df)\n",
    "df = StringIndexer(inputCol=\"WindDir3pm\", outputCol=\"WindDir3pmIndex\").fit(df).transform(df)\n",
    "df = StringIndexer(inputCol=\"RainToday\", outputCol=\"RainTodayIndex\").fit(df).transform(df)\n",
    "df = StringIndexer(inputCol=\"RainTomorrow\", outputCol=\"RainTomorrowIndex\").fit(df).transform(df)\n",
    "df = df.withColumn('WindGustDir', df['WindGustDirIndex'])\n",
    "df = df.withColumn('WindDir9am', df['WindDir9amIndex'])\n",
    "df = df.withColumn('WindDir3pm', df['WindDir3pmIndex'])\n",
    "df = df.withColumn('RainToday', df['RainTodayIndex'])\n",
    "df = df.withColumn('RainTomorrow', df['RainTomorrowIndex'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n",
      "DoubleType\n"
     ]
    }
   ],
   "source": [
    "for column in column_list:\n",
    "    print(df.schema[column].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDir|WindDir9am|WindDir3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+------------+\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+------------+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(*column_list)\n",
    "df.show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 07: Create the feature vector and divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-------------+---------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+------------+--------------------+-----+\n",
      "|MinTemp|MaxTemp|          Rainfall|WindGustSpeed|   WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDir|WindDir9am|WindDir3pm|RainToday|RainTomorrow|            features|label|\n",
      "+-------+-------+------------------+-------------+---------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+------------+--------------------+-----+\n",
      "|   13.4|   22.9|               0.6|         44.0|           20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|        1.0|       7.0|       7.0|      0.0|         0.0|[13.4,22.9,0.6,44...|  0.0|\n",
      "|    7.4|   25.1|               0.0|         44.0|            4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|        9.0|      10.0|       0.0|      0.0|         0.0|[7.4,25.1,0.0,44....|  0.0|\n",
      "|   12.9|   25.7|               0.0|         46.0|           19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|        0.0|       7.0|       0.0|      0.0|         0.0|[12.9,25.7,0.0,46...|  0.0|\n",
      "|    9.2|   28.0|               0.0|         24.0|           11.0|         9.0|       45.0|       16.0|     1017.6|     1012.8|       13.0|       2.0|      10.0|      0.0|         0.0|[9.2,28.0,0.0,24....|  0.0|\n",
      "|   17.5|   32.3|               1.0|         41.0|            7.0|        20.0|       82.0|       33.0|     1010.8|     1006.0|        1.0|      11.0|       8.0|      0.0|         0.0|[17.5,32.3,1.0,41...|  0.0|\n",
      "|   14.6|   29.7|               0.2|         56.0|           19.0|        24.0|       55.0|       23.0|     1009.2|     1005.4|        9.0|       7.0|       2.0|      0.0|         0.0|[14.6,29.7,0.2,56...|  0.0|\n",
      "|   14.3|   25.0|               0.0|         50.0|           20.0|        24.0|       49.0|       19.0|     1009.6|     1008.2|        1.0|       8.0|       2.0|      0.0|         0.0|[14.3,25.0,0.0,50...|  0.0|\n",
      "|    7.7|   26.7|               0.0|         35.0|            6.0|        17.0|       48.0|       19.0|     1013.4|     1010.1|        1.0|       4.0|       2.0|      0.0|         0.0|[7.7,26.7,0.0,35....|  0.0|\n",
      "|    9.7|   31.9|               0.0|         80.0|            7.0|        28.0|       42.0|        9.0|     1008.9|     1003.6|       14.0|       2.0|       8.0|      0.0|         1.0|[9.7,31.9,0.0,80....|  1.0|\n",
      "|   13.1|   30.1|               1.4|         28.0|           15.0|        11.0|       58.0|       27.0|     1007.0|     1005.7|        1.0|       6.0|       5.0|      1.0|         0.0|[13.1,30.1,1.4,28...|  0.0|\n",
      "|   13.4|   30.4|               0.0|         30.0|           17.0|         6.0|       48.0|       22.0|     1011.8|     1008.7|        4.0|       4.0|       9.0|      0.0|         1.0|[13.4,30.4,0.0,30...|  1.0|\n",
      "|   15.9|   21.7|               2.2|         31.0|           15.0|        13.0|       89.0|       91.0|     1010.5|     1004.2|       15.0|      13.0|      14.0|      1.0|         1.0|[15.9,21.7,2.2,31...|  1.0|\n",
      "|   15.9|   18.6|              15.6|         61.0|           28.0|        28.0|       76.0|       93.0|      994.3|      993.0|        1.0|      10.0|      13.0|      1.0|         1.0|[15.9,18.6,15.6,6...|  1.0|\n",
      "|   12.6|   21.0|               3.6|         44.0|           24.0|        20.0|       65.0|       43.0|     1001.2|     1001.8|        7.0|       7.0|      12.0|      1.0|         0.0|[12.6,21.0,3.6,44...|  0.0|\n",
      "|    9.8|   27.7|2.3499740743111954|         50.0|14.001988000994|        22.0|       50.0|       28.0|     1013.4|     1010.3|        9.0|       0.0|       7.0|      1.0|         0.0|[9.8,27.7,2.34997...|  0.0|\n",
      "|   14.1|   20.9|               0.0|         22.0|           11.0|         9.0|       69.0|       82.0|     1012.2|     1010.4|       11.0|      14.0|      10.0|      0.0|         1.0|[14.1,20.9,0.0,22...|  1.0|\n",
      "|   13.5|   22.9|              16.8|         63.0|            6.0|        20.0|       80.0|       65.0|     1005.8|     1002.2|        1.0|       1.0|       7.0|      1.0|         1.0|[13.5,22.9,16.8,6...|  1.0|\n",
      "|   11.2|   22.5|              10.6|         43.0|           24.0|        17.0|       47.0|       32.0|     1009.4|     1009.7|        5.0|       0.0|       4.0|      1.0|         0.0|[11.2,22.5,10.6,4...|  0.0|\n",
      "|    9.8|   25.6|               0.0|         26.0|           17.0|         6.0|       45.0|       26.0|     1019.2|     1017.1|        5.0|       2.0|      13.0|      0.0|         0.0|[9.8,25.6,0.0,26....|  0.0|\n",
      "|   11.5|   29.3|               0.0|         24.0|            9.0|         9.0|       56.0|       28.0|     1019.3|     1014.8|        6.0|       2.0|       1.0|      0.0|         0.0|[11.5,29.3,0.0,24...|  0.0|\n",
      "+-------+-------+------------------+-------------+---------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_list = ['MinTemp', \n",
    "               'MaxTemp', \n",
    "               'Rainfall', \n",
    "               'WindGustSpeed', \n",
    "               'WindSpeed9am', \n",
    "               'WindSpeed3pm', \n",
    "               'Humidity9am', \n",
    "               'Humidity3pm', \n",
    "               'Pressure9am', \n",
    "               'Pressure3pm',\n",
    "               'WindGustDir',\n",
    "               'WindDir9am',\n",
    "               'WindDir3pm',\n",
    "               'RainToday',               \n",
    "              ]\n",
    "\n",
    "\n",
    "va = VectorAssembler(inputCols=feature_list, outputCol='features')\n",
    "feature_df = va.transform(df)\n",
    "feature_df = feature_df.withColumn('label', feature_df['RainTomorrow'])\n",
    "feature_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = feature_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 08: Apply machine learning classification algorithms on the dataset and compare their accuracy. Plot the accuracy as bar graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'RainTomorrow', maxIter=10)\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "    \n",
    "# tvs = TrainValidationSplit(estimator=lr, trainRatio=0.8) # 80% for training, 20% for validation.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|[-8.5,0.6,1.0,31....|  0.0|       0.0|\n",
      "|[-8.2,15.2,0.0,17...|  0.0|       0.0|\n",
      "|[-8.0,2.1,2.34997...|  0.0|       0.0|\n",
      "|[-8.0,15.1,0.2,39...|  0.0|       0.0|\n",
      "|[-7.6,-1.4,5.8,31...|  0.0|       1.0|\n",
      "|[-7.5,-0.7,0.0,35...|  0.0|       0.0|\n",
      "|[-7.3,3.2,1.0,61....|  0.0|       1.0|\n",
      "|[-7.2,-1.7,0.8,57...|  1.0|       1.0|\n",
      "|[-7.0,0.1,0.8,19....|  0.0|       0.0|\n",
      "|[-7.0,10.5,0.0,28...|  0.0|       0.0|\n",
      "|[-6.9,11.4,0.0,37...|  0.0|       0.0|\n",
      "|[-6.9,14.0,0.0,30...|  0.0|       0.0|\n",
      "|[-6.8,13.8,0.0,39...|  0.0|       0.0|\n",
      "|[-6.8,14.5,5.8,48...|  0.0|       0.0|\n",
      "|[-6.7,2.9,0.8,33....|  0.0|       0.0|\n",
      "|[-6.7,15.7,0.0,37...|  0.0|       0.0|\n",
      "|[-6.6,1.1,0.0,41....|  0.0|       0.0|\n",
      "|[-6.6,1.4,0.2,30....|  0.0|       0.0|\n",
      "|[-6.5,-3.8,8.4,52...|  1.0|       1.0|\n",
      "|[-6.5,-2.0,4.6,57...|  0.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-203c4e5efadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lrModel' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.select(feature_list).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0d1b36e012af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     metricName=\"accuracy\")\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy: %f\" % (accuracy))\n",
    "print(\"Test Error: %f\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 09: Calculate the confusion matrix and find the precision, recall, and F1 score of each classification algorithm. Explain how the accuracy of the predication can be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_temp = predictions.select(\"label\").groupBy(\"label\")\\\n",
    "                        .count().sort('count', ascending=False).toPandas()\n",
    "class_temp = class_temp[\"label\"].values.tolist()\n",
    "class_names = map(str, class_temp)\n",
    "# # # print(class_name)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = predictions.select(\"label\")\n",
    "y_true = y_true.toPandas()\n",
    "\n",
    "y_pred = predictions.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred) # labels=class_names\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
